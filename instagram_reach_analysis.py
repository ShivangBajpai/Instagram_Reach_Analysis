# -*- coding: utf-8 -*-
"""Instagram_Reach_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13yXj3yrFckLLo3lkadJ3gLYz725DVlc3
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

x=pd.read_csv('/content/Instagram_Data.csv',encoding="ISO-8859-1")
x

x.head()

x.tail()

x.info()

x.describe().T

x.shape

x.isnull().sum()

x.duplicated().sum()

(x.drop_duplicates(inplace=True))
if x.duplicated().sum()==0:
  print("No Duplicates")
else:
  x.drop_duplicates(inplace=True)
  print("All Duplicates Deleted")
print(x)

x.dtypes

x.drop(["Caption"],axis=1,inplace=True)

x.columns

#Finding the outliers
sns.histplot(x,x="Impressions")
plt.title("Distribution of Impressions Count")
plt.show()

#Finding the outliers
sns.histplot(x,x="From Home")
plt.title("Distribution of views from Home Icon")
plt.show()

#Finding the outliers
sns.histplot(x,x="Likes")
plt.title("Distribution of Likes Count")
plt.show()

#Finding the outliers
sns.histplot(x,x="Comments")
plt.title("Distribution of Comments Count")
plt.show()

#Finding the outliers
sns.histplot(x,x="Shares")
plt.title("Distribution of Shares Count")
plt.show()

#Finding the outliers
sns.histplot(x,x="From Hashtags")
plt.title("Distribution of Hashtags Count")
plt.show()

#Finding the outliers
sns.histplot(x,x="From Explore")
plt.title("Distribution of explore Count")
plt.show()

#Finding the outliers
sns.histplot(x,x="From Other")
plt.title("Distribution of Other Count")
plt.show()

#Finding the outliers
sns.histplot(x,x="Saves")
plt.title("Distribution of Saves Count")
plt.show()

#Finding the outliers
sns.histplot(x,x="Follows")
plt.title("Distribution of Followers Count")
plt.show()

#Finding the outliers
sns.histplot(x,x="Profile Visits")
plt.title("Distribution of Profile Visits Count")
plt.show()

a=x["Impressions"].mean()
b=x["Likes"].mean()
c=x["Comments"].mean()
d=x["Shares"].mean()
e=x["Saves"].mean()
f=x["Follows"].mean()
g=x["Profile Visits"].mean()
h=x["From Home"].mean()
i=x["From Hashtags"].mean()
j=x["From Explore"].mean()
k=x["From Other"].mean()

a,b,c,d,e,f,g,h,i,j,k

plt.figure(figsize=(25,7))
plt.bar(x=["Impressions","Likes","Comments","Shares","Saves","Follows","Profile Visits","From Home","From Hashtags","From Explore", "From Other"],
            height=["a","b","c","d","e","f","g","h","i","j","k"])
plt.show()

x.reset_index(inplace=True)

x

x.nunique()

plt.plot(x.index,x["Impressions"],"bs-")
plt.show()

x[x["Impressions"]>15000]

#Engagement Ratio
x["Engagement Ratio"]=(x["Likes"  ]+x["Comments"]+x["Shares"]+x["Saves"]+x["Profile Visits"])/x["Impressions"]

x

x.drop(["Hashtags"],axis=1,inplace=True)

x

#Correlation between Columns
plt.figure(figsize=(20,20))
sns.heatmap(data=x.corr(),annot=True,cmap="coolwarm")
plt.show()

#Again loading the Original Data
x=pd.read_csv('/content/Instagram_Data.csv',encoding="ISO-8859-1")

x.columns

#Again Removing Duplicates
(x.drop_duplicates(inplace=True))
if x.duplicated().sum()==0:
  print("No Duplicates")
else:
  x.drop_duplicates(inplace=True)
  print("All Duplicates Deleted")
print(x)

#Again Reseting Index
x.reset_index(inplace=True)

#Engagement Ratio
x["Engagement Ratio"]=(x["Likes"  ]+x["Comments"]+x["Shares"]+x["Saves"]+x["Profile Visits"])/x["Impressions"]

x

pip install wordcloud

hashtag=" ".join(x["Hashtags"].tolist())
wordcloud=WordCloud(width=1000,height=500,background_color="white",min_font_size=10).generate(hashtag)
plt.figure(figsize=(15,8))
plt.imshow(wordcloud)
plt.axis("on")
plt.show()

wordcloud=WordCloud(background_color="black",min_font_size=10).generate(hashtag)
plt.figure(figsize=(15,8))
plt.imshow(wordcloud)
plt.axis("on")
plt.show()

hashtag













